{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNvyk6StKu31RCb1CpkjalW",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Gemlala/AI-Projekt/blob/main/Image_Captioning_f%C3%BCr_Kreiselbilder_(PyTorch).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "üöÄ Notebook-Ger√ºst: Image Captioning f√ºr Kreiselbilder (PyTorch)"
      ],
      "metadata": {
        "id": "YiA01eMfCEQw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CvEr2PqE5UHu",
        "outputId": "327a1891-2245-4853-c735-51a9576358c7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WZ2qKxmlB--t",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 905
        },
        "outputId": "9b848fcb-ea6e-4d9a-acee-b01ea1f58710"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (2.9.0+cu126)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.12/dist-packages (0.24.0+cu126)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.12/dist-packages (11.3.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (2.2.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (2.0.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (4.67.1)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.12/dist-packages (4.57.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch) (3.20.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch) (1.14.0)\n",
            "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch) (3.6)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec>=0.8.5 in /usr/local/lib/python3.12/dist-packages (from torch) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch) (2.27.5)\n",
            "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch) (3.3.20)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.5.0 in /usr/local/lib/python3.12/dist-packages (from torch) (3.5.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.36.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from transformers) (6.0.3)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2025.11.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from transformers) (2.32.4)\n",
            "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.22.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.7.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.2.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch) (3.0.3)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2025.11.12)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'cpu'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "# ================================================\n",
        "# 0. SETUP\n",
        "# ================================================\n",
        "!pip install torch torchvision pillow pandas numpy tqdm transformers\n",
        "\n",
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from PIL import Image\n",
        "from tqdm import tqdm\n",
        "from transformers import AutoTokenizer, VisionEncoderDecoderModel, AutoFeatureExtractor\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "device\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "csv_path = \"/content/drive/MyDrive/Colab Notebooks/Bsc/gdf_u_filtered_subset.csv\"\n",
        "\n",
        "# CSV einlesen\n",
        "df = pd.read_csv(csv_path)\n",
        "\n",
        "# Die ersten 5 Zeilen anzeigen\n",
        "print(df.head())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "obSm4b07M5tz",
        "outputId": "eb7cb57a-7283-46ea-bee9-9dd1a6e2486c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   Unfall-Nr (UAP) Unfalldatum (UAP)  \\\n",
            "0          1145168        2021-03-16   \n",
            "1           243027        2023-07-19   \n",
            "2              663        2023-06-09   \n",
            "3     620201000197        2020-10-29   \n",
            "4     202307001851        2023-07-09   \n",
            "\n",
            "                                     Unfalltyp (UAP)  \\\n",
            "0  Kollision beim Rechtseinbiegen mit von links k...   \n",
            "1                       Anderer Unfall beim Abbiegen   \n",
            "2  Kollision beim Rechtseinbiegen mit von links k...   \n",
            "3  Kollision beim Rechtseinbiegen mit von links k...   \n",
            "4                                     Ohne Kollision   \n",
            "\n",
            "               Unfalltyp Gruppe  Anzahl beteiligte Fahrr√§der (UAP-Objekt)  \\\n",
            "0                Einbiegeunfall                                         0   \n",
            "1                 Abbiegeunfall                                         1   \n",
            "2                Einbiegeunfall                                         0   \n",
            "3                Einbiegeunfall                                         0   \n",
            "4  Schleuder- oder Selbstunfall                                         0   \n",
            "\n",
            "   Anzahl beteiligte Fahrr√§der mit elektrischer Tretunterst√ºtzung (bis 25 km/h) (UAP-Objekt)  \\\n",
            "0                                                  0                                           \n",
            "1                                                  0                                           \n",
            "2                                                  0                                           \n",
            "3                                                  0                                           \n",
            "4                                                  0                                           \n",
            "\n",
            "   Anzahl beteiligte Fahrr√§der mit elektrischer Tretunterst√ºtzung (bis 45 km/h mit gelbem Schild/Helmpflicht) (UAP-Objekt)  \\\n",
            "0                                                  0                                                                         \n",
            "1                                                  0                                                                         \n",
            "2                                                  0                                                                         \n",
            "3                                                  0                                                                         \n",
            "4                                                  0                                                                         \n",
            "\n",
            "   Unfallstelle (UAP)  Koordinate E  Koordinate N  ...  \\\n",
            "0  Kreisverkehrsplatz   2588421.109   1223213.805  ...   \n",
            "1  Kreisverkehrsplatz   2679938.276   1226737.004  ...   \n",
            "2  Kreisverkehrsplatz   2519735.530   1161123.766  ...   \n",
            "3  Kreisverkehrsplatz   2662015.718   1194362.647  ...   \n",
            "4  Kreisverkehrsplatz   2626076.242   1228603.516  ...   \n",
            "\n",
            "  Unfalldatum / Unfallzeit Vortrittsregelung (UAP) Witterung (UAP)  \\\n",
            "0         16.03.2021 10:25  Signal Kein Vortritt\"\"         bedeckt   \n",
            "1         19.07.2023 09:15                  andere         bedeckt   \n",
            "2         09.06.2023 12:20  Signal Kein Vortritt\"\"          sch√É¬∂n   \n",
            "3         29.10.2020 21:30  Signal Kein Vortritt\"\"         bedeckt   \n",
            "4         09.07.2023 11:25  Signal Kein Vortritt\"\"          sch√É¬∂n   \n",
            "\n",
            "  Strassenart (UAP) Kinderunfall auf dem Schulweg Kinderunfall Seniorenunfall  \\\n",
            "0      Hauptstrasse                          Nein         Nein             Ja   \n",
            "1      Nebenstrasse                          Nein         Nein             Ja   \n",
            "2      Hauptstrasse                          Nein         Nein           Nein   \n",
            "3      Hauptstrasse                          Nein         Nein             Ja   \n",
            "4      Hauptstrasse                          Nein         Nein             Ja   \n",
            "\n",
            "   Sachschaden (in CHF) (UAP)  Total Personen (UAP)  index_righ  \n",
            "0                        4000                     3        3131  \n",
            "1                         300                     1        2584  \n",
            "2                        4000                     3        1874  \n",
            "3                        5000                     2        1190  \n",
            "4                        1000                     3        3039  \n",
            "\n",
            "[5 rows x 66 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1Ô∏è‚É£ Daten vorbereiten\n",
        "\n",
        "Angenommen:\n",
        "\n",
        "deine Bilder liegen hier:\n",
        "/content/drive/MyDrive/Colab Notebooks/Bsc/SwissImageTiles_Kreisel_JPG\n",
        "\n",
        "die Bildnamen sind: 0.jpg, 1.jpg, ..., 3369.jpg\n",
        "\n",
        "du hast eine CSV mit Beschreibungen (Caption-Text)\n",
        "\n",
        "Beispiel-CSV:\n",
        "\n",
        "id\tcaption\n",
        "0\t\"Ein grosser Kreisel mit vier Zufahrten...\"\n",
        "\n",
        "Falls du die Caption-Texte erst generieren willst ‚Üí sag Bescheid."
      ],
      "metadata": {
        "id": "cDRFh50mCpma"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ================================================\n",
        "# 1. DATEN LADEN\n",
        "# ================================================\n",
        "\n",
        "IMG_DIR = \"/content/drive/MyDrive/Colab Notebooks/Bsc/SwissImageTiles_Kreisel_PNG_A\"\n",
        "CAPTION_FILE = \"/content/drive/MyDrive/Colab Notebooks/Bsc/captions.csv\"\n",
        "\n",
        "df = pd.read_csv(CAPTION_FILE)\n",
        "df.head()\n"
      ],
      "metadata": {
        "id": "eu6KG-a4CsG1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2Ô∏è‚É£ Dataset & Dataloader"
      ],
      "metadata": {
        "id": "atbjPErCCHyc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ================================================\n",
        "# 2. DATASET DEFINIEREN\n",
        "# ================================================\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
        "# GPT-2 ben√∂tigt pad_token\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "class RoundaboutCaptionDataset(Dataset):\n",
        "    def __init__(self, df, img_dir, feature_extractor, tokenizer):\n",
        "        self.df = df\n",
        "        self.img_dir = img_dir\n",
        "        self.feature_extractor = feature_extractor\n",
        "        self.tokenizer = tokenizer\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        row = self.df.iloc[idx]\n",
        "        img_id = row[\"id\"]\n",
        "        caption = row[\"caption\"]\n",
        "\n",
        "        img = Image.open(os.path.join(self.img_dir, f\"{img_id}.jpg\")).convert(\"RGB\")\n",
        "\n",
        "        pixel_values = self.feature_extractor(images=img, return_tensors=\"pt\")[\"pixel_values\"].squeeze()\n",
        "\n",
        "        encoded = tokenizer(\n",
        "            caption,\n",
        "            padding=\"max_length\",\n",
        "            max_length=64,\n",
        "            truncation=True,\n",
        "            return_tensors=\"pt\"\n",
        "        )\n",
        "\n",
        "        return {\n",
        "            \"pixel_values\": pixel_values,\n",
        "            \"input_ids\": encoded[\"input_ids\"].squeeze(),\n",
        "            \"attention_mask\": encoded[\"attention_mask\"].squeeze()\n",
        "        }\n",
        "\n",
        "feature_extractor = AutoFeatureExtractor.from_pretrained(\"google/vit-base-patch16-224-in21k\")\n",
        "\n",
        "dataset = RoundaboutCaptionDataset(df, IMG_DIR, feature_extractor, tokenizer)\n",
        "dataloader = DataLoader(dataset, batch_size=8, shuffle=True)\n"
      ],
      "metadata": {
        "id": "eBhYDQeOCmLj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "3Ô∏è‚É£ Modell definieren\n",
        "\n",
        "Wir nutzen ein VisionEncoderDecoder-Modell:\n",
        "\n",
        "Encoder: ViT (Vision Transformer)\n",
        "\n",
        "Decoder: GPT-2"
      ],
      "metadata": {
        "id": "-o9SYEAACvda"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ================================================\n",
        "# 3. MODELL\n",
        "# ================================================\n",
        "\n",
        "model = VisionEncoderDecoderModel.from_encoder_decoder_pretrained(\n",
        "    \"google/vit-base-patch16-224-in21k\",\n",
        "    \"gpt2\"\n",
        ")\n",
        "\n",
        "model.config.decoder_start_token_id = tokenizer.bos_token_id\n",
        "model.config.pad_token_id = tokenizer.pad_token_id\n",
        "\n",
        "model.to(device)\n"
      ],
      "metadata": {
        "id": "00EhKAekCw5W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "4Ô∏è‚É£ Training Loop"
      ],
      "metadata": {
        "id": "a14f2cUJCycO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ================================================\n",
        "# 4. TRAINING LOOP\n",
        "# ================================================\n",
        "\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-5)\n",
        "\n",
        "EPOCHS = 3\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "    model.train()\n",
        "    pbar = tqdm(dataloader, desc=f\"Epoch {epoch+1}/{EPOCHS}\")\n",
        "\n",
        "    for batch in pbar:\n",
        "        pixel_values = batch[\"pixel_values\"].to(device)\n",
        "        input_ids = batch[\"input_ids\"].to(device)\n",
        "        attention_mask = batch[\"attention_mask\"].to(device)\n",
        "\n",
        "        outputs = model(\n",
        "            pixel_values=pixel_values,\n",
        "            labels=input_ids,\n",
        "            attention_mask=attention_mask\n",
        "        )\n",
        "\n",
        "        loss = outputs.loss\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        pbar.set_postfix({\"loss\": loss.item()})\n"
      ],
      "metadata": {
        "id": "Bb5bqEweCzrT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "5Ô∏è‚É£ Caption generieren"
      ],
      "metadata": {
        "id": "admT5aFkC1iS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ================================================\n",
        "# 5. INFERENCE\n",
        "# ================================================\n",
        "\n",
        "def generate_caption(img_path):\n",
        "    img = Image.open(img_path).convert(\"RGB\")\n",
        "    pixel_values = feature_extractor(images=img, return_tensors=\"pt\")[\"pixel_values\"].to(device)\n",
        "\n",
        "    output_ids = model.generate(\n",
        "        pixel_values,\n",
        "        max_length=64,\n",
        "        num_beams=4\n",
        "    )[0]\n",
        "\n",
        "    caption = tokenizer.decode(output_ids, skip_special_tokens=True)\n",
        "    return caption\n",
        "\n",
        "# Beispiel\n",
        "generate_caption(os.path.join(IMG_DIR, \"10.jpg\"))\n"
      ],
      "metadata": {
        "id": "YlT-SXklC26C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "6Ô∏è‚É£ Optional: qualitative Evaluation"
      ],
      "metadata": {
        "id": "oDDaQwoKC4eu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "img_id = 42\n",
        "img = Image.open(os.path.join(IMG_DIR, f\"{img_id}.jpg\"))\n",
        "plt.imshow(img)\n",
        "plt.axis(\"off\")\n",
        "print(\"Predicted:\", generate_caption(os.path.join(IMG_DIR, f\"{img_id}.jpg\")))\n",
        "print(\"GT:\", df[df.id == img_id].caption.values[0])\n"
      ],
      "metadata": {
        "id": "lItCtnj1C509"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "üéâ Fertig!\n",
        "\n",
        "Damit hast du den vollst√§ndigen Rahmen f√ºr ein Image Captioning Projekt mit Kreiselbildern:\n",
        "\n",
        "ViT + GPT-2 Modell\n",
        "\n",
        "Trainingsloop\n",
        "\n",
        "Caption-Inference\n",
        "\n",
        "Bild-Ausgabe\n",
        "\n",
        "Willst du als N√§chstes:\n",
        "\n",
        "‚úÖ automatisch Caption-Texte aus Unfalldaten generieren?\n",
        "‚úÖ die Captions zu Risikoklassen (low/medium/high) erweitern?\n",
        "‚úÖ statt GPT-2 ein LLama-Decoder nutzen?"
      ],
      "metadata": {
        "id": "cHs1sudUC7wA"
      }
    }
  ]
}